---
title: "Regression Analysis Examples in R"
fontsize: 11pt 
geometry: margin=1in
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{mathtools}
   - \usepackage{enumitem}
   - \usepackage{setspace}
   - \usepackage{amsthm}
   - \usepackage{centernot}
   - \DeclareMathOperator\arctanh{arctanh}
   - \DeclareMathOperator{\Unif}{Unif}
   - \DeclareMathOperator{\Var}{Var}
   - \DeclareMathOperator{\Cov}{Cov}
   - \DeclareMathOperator{\Bern}{Bernoulli}
   - \DeclareMathOperator{\Bin}{Bin}
   - \DeclareMathOperator{\Pois}{Poisson}
   - \newtheorem{theorem}{Theorem}[section]
   - \newtheorem{definition}{Definition}[section]
   - \newtheorem{fact}{Fact}[section]
   - \newtheorem{prop}{Proposition}[section]
   - \newtheorem{corollary}{Corollary}[section]
   - \newtheorem{lemma}{Lemma}[section]
   - \newcommand{\iid}[0]{\overset{\mathrm{iid}}{\sim}}
output:
  pdf_document:
    toc_depth:
    number_sections: false
  fig_caption: yes
---

```{r setup, include=FALSE}
```

# Chapter 2 - Simple Linear Regression

## Example. Airfreight Data 

\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
        \hline
        Shipment Route ($x$) & 1 & 0 & 2 & 0 & 3 & 1 & 0 & 1 & 2 & 0\\
        Airfreight Breakage ($y$) & 16 & 9 & 17 & 12 & 22 & 13 & 8 & 15 & 19 & 11\\
        \hline
    \end{tabular}
\end{center}
 a) Compute the ANOVA table
 b) Compute the confidence intervals for the parameters 
 c) Compute the confidence interval on the average (mean) response when $X = 1$. 
 d) What is the total variability in $y$ explained by this model?

## Solution. 

### Part a.
We can compute the anova table manually as follows, 

\[S_{xx} = \sum_{i=1}^n (x_i-\bar{x})^2 = \sum_{i=1}^n x_i^2 - \frac{1}{n}\left(\sum_{i=1}^n x_i\right)^2 = 20 - \frac{1}{10}(100) = 10\]
\[S_{xy} = \sum_{i=1}^n y_i(x_i - \bar{x}) = \sum_{i=1}^n x_iy_i - \frac{1}{n}\sum_{i=1}^n x_i \sum_{i=1}^n y_i  = 182 - \frac{1}{10}(10)(142) = 40\]
Therefore,
\[\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} = \frac{40}{10} = 4\]
Then, $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}$, so 
\[\hat{\beta}_0 = \frac{1}{10}(142) - 4\cdot \frac{1}{10}(10) = 10.2\]
This gives us our linear model 
\[\hat{y} = 10.2 + 4x\]
The sum of squares for regression is 
\[SSR = \hat{\beta}_1^2 \sum_{i=1}^n (x_i - \bar{x})^2 = \hat{\beta}_1^2 S_{xx} = 16 \cdot 10 = 160 \]
The total sum of squares is 
\[SST = \sum_{i=1}^n (y_i-\bar{y})^2 = \sum_{i=1}^n y_i^2 - n\bar{y}^2 =  2194 - 10(14.2)^2 = 177.6\]
Then, the residual sum of squares is 
\[SSE = SST - SSR = 177.6 - 160 = 17.6\]
Now we can construct the anova table 
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Source & Sum of Squares & DF & MS=SS/df & F = MSR/MSE\\
        \hline
        Regression & 160 & 1 & 160 & 72.727  \\
        Error & 17.6 & 8 & 2.2 & \\
        Total & 177.6 & & & \\
        \hline
    \end{tabular}
\end{center}
We conclude tbat the regression is highly significant since the $F$ value is very large. We can also do this in R 
```{r}
x <- c(1,0,2,0,3,1,0,1,2,0)
y <- c(16,9,17,12,22,13,8,15,19,11)
model <- lm(formula = y ~ x)
print(model)
anova(model)
```
We can see that we get the same results and reach the same conclusion, and we also get the $p$-value which is very small and we can conclude from there as well that the regression is highly significant.

### Part b.

We can construct confidence intervals, first we need to compute $se(\hat{\beta}_1)$ and $se(\hat{\beta}_0)$. 
\[se^2(\hat{\beta}_0) = MSE\left(\frac{1}{n} + \frac{\bar{x}}{S_{xx}}\right) = 2.2\left(\frac{1}{10} + \frac{1}{10}\right) = 0.44 \implies se(\hat{\beta}_0) = \sqrt{0.44} = 0.6633\]
\[se^2(\hat{\beta}_1) = \frac{MSE}{S_{xx}} = \frac{2.2}{10} = 0.22 \implies se(\hat{\beta}_1) = \sqrt{0.22} = 0.490\]
Then, we have to compute $t_{\alpha/2, n-2} = t_{0.025, 8}$, we either use a $t$ look up table or in R, 
```{r}
qt(0.025, 8, lower.tail=FALSE)
```
Thus, our confidence intervals are 
\[\hat{\beta}_0 - t_{\alpha/2, n-2}se(\hat{\beta}_0) \leq \hat{\beta}_0 \leq \hat{\beta}_0 + t_{\alpha/2, n-2}se(\hat{\beta}_0) \rightarrow 10.2 \pm 2.306(0.6633) = (8.6704 ,11.7296)\]
\[\hat{\beta}_1 - t_{\alpha/2, n-2}se(\hat{\beta}_1) \leq \hat{\beta}_1 \leq \hat{\beta}_1 + t_{\alpha/2, n-2}se(\hat{\beta}_1) \rightarrow 4 \pm 2.306(0.490) = (2.9392,5.0608)\]
We can compute these confidence intervals in R as well 
```{r}
confint(model, level=0.95)
```

### Part c.

We want to compute first $E(y|x_0)$, where $x_0 = 1$. An unbiased estimator for $E(y|x_0)$ is 
\[\widehat{E(y|x_0)} = \hat{\mu}_{y|x_0} = \hat{\beta}_0 + \hat{\beta}_1x_0 = 10.2 + 4(1) = 14.2\]
Then, the confidence interval is 
\[\left[\hat{\mu}_{y|x_0} \pm t_{\alpha/2, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right)} \right] = \left[14.2 \pm 2.306\sqrt{2.2\left(\frac{1}{10} + \frac{(1-1)^2}{S_{xx}}\right)}\right] = (13.11839, 15.28161)\]
We can do this in R with 
```{r}
predict(model, newdata = data.frame(x=1), interval = 'confidence', level=0.95)
```

### Part d. 

The total variability in $y$ explained by the regressor $x$ is measured by the coefficient of determination 
\[R^2 = \frac{SSR}{SST} = \frac{160}{177.6} = 0.9009\]
We can also see this in the summary of the model in R

```{r}
summary(model)
```
The $R^2$ value is 0.9009.

# Chatper 3 - Multiple Linear Regression 

## Question 3.1

Consider the National Football League data in Table B.1

  a. Fit a multiple linear regression model relating the number of games won to the team's passing yardage ($x_2$), the percentage of rushing plays ($x_7$), and the opponents' yards rushing ($x_8$). 
  b. Construct the analysis-of-variance table and test for significance of regression.
  c. Calculate $t$ statistics for the hypotheses $H_0: \beta_2 = 0$, $H_0: \beta_7 = 0$, and $H_0: \beta_8 = 0$. What conclusions can you draw about the roles of variables in $x_2$, $x_7$, and $x_8$ play in the model?
  d. Calculate $R^2$ and $R^2_{Adj}$ for this model.
  e. Using the partial $F$ test, determine the contribution of $x_7$ to the model. How is the partial $F$ statistic related to the $t$ test for $\beta_7$ calculated in part c above?

## Question 3.3
Refer to problem 3.1
\begin{enumerate}[label=\alph*.]
  \item Find a 95\% CI for $\beta_7$. 
  \item Find a 95\% CI on the mean number of games won by a team when $x_2 = 2300$, $x_7 = 56$, and $x_8 = 2100$.
\end{enumerate}



## Question 3.4

Reconsider the National Football League data from Problem 3.1. Fit a model to these data using only $x_7$ and $x_8$ as regressors.
\begin{enumerate}[label=\alph*.]
  \item Test for significance of regression.
  \item Calculate $R^2$ and $R^2_{Adj}$. How do these quantities compare to the value computed for the model in Problem 3.1, which included an additional regressor ($x_2$)?
  \item Calculate a 95\% CI on $\beta_7$. Also find a 95\% CI on the mean number of games won by a team when $x_7 = 56$, and $x_8 = 2100$. Compare the lengths of these CIs to the lengths of the corresponding CIs from Problem 3.3.
  \item What conclusions can you draw from this problem about the consequences of omitting an important regressor from a model?
\end{enumerate}


## Solutions.

```{r, echo=FALSE, message=FALSE} 
# Loading necessary libraries and preparing data for question 2.1.
library(MPV)
library(ggplot2)
data(table.b1)
tableb1 <- table.b1
```

## Question 3.1 

### Part a. 
We can fit a linear model using the same R function, 
```{r}
  # Table b1 was loaded ahead of time.
  model <- lm(formula = y ~ x2 + x7 + x8, tableb1)
  model
```
This gives us our linear model with the estimates $\hat{\beta}_0 = -1.808$, $\hat{\beta_2} = 0.00360$, $\hat{\beta}_7 = 0.194$, and $\hat{\beta}_8 = -0.00482$. 
\[y = -1.808 + 0.00360x_2 + 0.194x_7 -0.00482x_8\]

### Part b.

We test for significance of regession using the hypotheses
\[H_0: \beta_2 = \beta_7 = \beta_8 = 0, \ H_1: \beta_j \neq 0, j = 2,7,8\]
We use the $F$-statistic
\[F_0 = \frac{SSR/k}{SSE/(n-p)} = \frac{MSR}{MSE}\sim F_{k,n-p}\]
We reject the null hypothesis when $F > F_{\alpha, k, n-k-1}$, we compute these values in R. In this case, we have $k=3$ regressors and coefficients, so $p = k+1 = 4$, thus

```{r}
n <- nrow(tableb1)
anova(model)
qf(0.05, 3, n-4, lower.tail=FALSE)
```
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      Source & Sum of Squares & DF & MS & F & $P$ \\ 
      \hline
      $x_2$ & 76.193 & 1 & 76.193 &26.172  &$3.1 \cdot 10^{-5}$ \\
      $x_7$ & 139.501 & 1 & 139.501 & 47.918 & $3.698 \cdot 10^{-7}$ \\
      $x_8$ & 41.400 & 1 & 41.400 & 14.221 & 0.0009378\\
      Residuals & 69.8790 & 24 & 2.911 & & \\
      \hline
  \end{tabular}
\end{center}
\[F_0 = \frac{SSR/k}{SSE/(n-p)} = \frac{(76.193 + 139.501 + 41.4)/3}{69.870/24} = 29.439\]

We can also obtain the F-statistic from the summary of the model, 
```{r}
summary(model)
```
Therefore, we reject the null hypothesis $H_0: \beta_2 = \beta_7 = \beta_8 = 0$, and conclude our regression is significant.

### Part c.

We want to conduct tests on the indivudal coefficients, with the hypotheses $H_0: \beta_2 = 0$, $H_0: \beta_7 = 0$, and $H_0: \beta_8 = 0$. We need the $t$-statistic
\[t_0 = \frac{\hat{\beta}_j}{se(\hat{\beta}_j)}\]
We reject the null hypothesis when $|t_0| > t_{\alpha/2, n-p}$, 
```{r}
qt(0.025, n-4, lower.tail=FALSE)
```
We have all the information we need however in the summary of the model, we can see the estimate and the standard error for each coefficient, which tells us the $t$-value, but also the $t$-value is included. We can see that for all coefficents and their respective $t$-values, $|t_0| > t_{\alpha/2, n-p}$ so we reject all 3 of the null hypotheses.


### Part d. 

The $R^2$ value can be computed with 
\[R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} = \frac{76.193 + 139.501 + 41.4}{76.193 + 139.501 + 41.4 + 69.8790} = 0.7863\]
We get these values from the ANOVA table above and also in the summaryof our model, we have $R^2 =0.7863$, and the adjusted $R^2$ value is 
\[R^2_{Adj} = 1 - \frac{SSE/(n-k)}{SST/(n-1)} = 1 - \frac{68.8790/24}{326.973/27} = 0.7596\]
This value is also in the summary R output above.


### Part e. 

To conduct the partial $F$ test to determine the contribution of $x_7$, we want to tes tthe hypotheses
\[H_0: \beta_7 = 0, \ H_1: \beta_7 \neq 0\]
We fit the model assuming the null hypothesis is true to get the reduced model and obtain the anova table,
```{r}
reduced_model <- lm(formula = y ~ x8 + x2, data=tableb1)
anova(reduced_model)
```
Then, we want to use the $F$ statistic to test the hypotheses
\[F_0 = \frac{SSR(\beta_7| \beta_2, \beta_0)/r}{MSE}\]
So, we have 
\[SRR(\beta_7|\beta_2, \beta_8) = SSR(\beta_7,\beta_2,\beta_8) - SSR(\beta_2, \beta_8) = 257.094 - (178.092 + 64.934) = 14.064\]
Therefore,
\[F_0 = \frac{SSR(\beta_7| \beta_2, \beta_8)}{MSE} = \frac{14.064}{2.911} \approx 4.831\]
we reject the null hypothesis if $F_0 > F_{\alpha, r, n-p}$, 
```{r}
qf(0.05, 1, 24, lower.tail=FALSE)
```
We conclude that $x_7$ contributed significantly to this model. We also notice that the $F$ statistic is the square of the $t$ test used in part c.

## Question 3.3 

### Part a. 
To consturct a confidence interval on $\beta_7$, we need to compute 
\[\hat{\beta}_j - t_{\alpha/2,n-p}\sqrt{\hat{\sigma}^2C_{jj}} \leq \beta_j \leq \hat{\beta}_j - t_{\alpha/2,n-p}\sqrt{\hat{\sigma}^2C_{jj}}\]

We have the standard error for $\beta_7$ from the previous summary output from R, so $se(\hat{\beta}_j) = \sqrt{\hat{\sigma}^2C_{jj}} = 0.088233$. Then, the $t$-value was also obtained earlier and we have $t_{\alpha/2, n-p} = 2.063899$, and the coefficient estimator $\hat{\beta_7} = 0.193960$. Therefore, the confidence interval is 
\[0.193960 \pm 2.063899 \cdot(0.088233) = (0.011856, 0.376064)\]
This can be also obtained in R 
```{r}
confint(model, "x7")
```

### Part b.

The mean response at $x_2 = 2300$, $x_7 = 56$ and $x_8 = 2100$ is 

\[y_0 = \beta_0 + \beta_2 (2300) + \beta_7 (56) + \beta_8 (2100) = 7.215188\]

Then, the confidence interval on the mean response is 
\[\left[\hat{y}_0 \pm t_{\alpha/2,n-p}\sqrt{\hat{\sigma}^2x_0'(X'X)^{-1}x_0}\right]\]
This can be calculated in R with 

```{r}
predict(model, newdata=data.frame(x2 = 2300, x7 = 56, x8 = 2100),
        interval = 'confidence', level=0.95)
```

## Question 3.4

### Part a 
We want to test the hypotheses for significance, 
\[H_0: \beta_7 = \beta_8 = 0, \ H_1: \beta_j \neq 0, \ j = 7,8\]
We can fit the model and get the anova table, 
```{r}
model <- lm(formula = y ~ x7 + x8, tableb1)
anova(model)
```
We can see from the anova table that the regression is highly significant, but we can also use the $F$ test statistic, 
\[F_0 = \frac{SSR/k}{SSE/(n-k-1)} = \frac{(97.238 + 81.828)/2}{147.898/25} = \frac{89.533}{5.916} = 15.134\]
Then, we can compute $F_{\alpha, k, n-k-1}$, 
```{r}
qf(0.05, 3, 25, lower.tail=FALSE)
```
We can see that $F_0 > F_{\alpha, k, n-k-1}$ so we reject the null hypotheses that $H_0: \beta_7 = \beta_8 = 0$, and conclude that the regression is significant.

### Part b.
We can obtain a summary for the model and look at the $R^2$ and $R_{Adj}^2$ values similar to the previous questions.
```{r}
summary(model)
```

We get an R-squared value $R^2 = 0.5477$ and the adjusted R-squared is $R^2_{Adj} = 0.5115$, we can see that these values are lower than when we had $x_2$ in the model. So, the model with $x_2$ was able to better explain the variablity in $y$ and this suggests $x_2$ may have been contributing significantly to the model.

### Part c.
```{r}
confint(model, "x7")
```
A 95\% confidence interval on $\beta_7$ is 
\[(-0.1971643, 0.293906)\]

```{r}
new_data <- data.frame(x7 = 56, x8=2100)
predict(model, newdata=new_data, interval='confidence', level=0.95)
```
Our 95\% confidence interval on the mean number of games one when $x_7 = 56$ and $x_8 = 2100$ is 
\[(5.828643, 8.023842)\]
We can see that the length of both confidence intervals are greater than when $x_2$ was included in the model. This suggests we were more confident with our estimates when $x_2$ was included.

### d.

We can conclude that omitting an important regressor ($x_2$) affected our estimates and standard error of coeffficients, resulting in larger lenghts in the confidence intervals and lower values for $R^2$.


# Chapter 4 - Model Adequecy


## Question 4.1

Consider the simple regression model fit to the National Football League team performance data in Problem 2.1. (Same data as previous questions, with the model $y ~ x8$.

\begin{enumerate}[label=\alph*.]
  \item Construct a normal probability plot of the residuals. Does there seem to be any problem with the normality assumption? 
  \item Construct an interpret a plot of the residuals versus the predicted repsonse. 
  \item Plot the residuals versus the team passing yardage, $x_2$. DOes this plot indicate that the model will be improved by adding $x_2$ to the model?
\end{enumerate}



### Part a.
```{r, out.width="90%"}
library(ggplot2)
model <- lm(formula = y ~ x8, tableb1)
residuals <- residuals(model)
qqnorm(residuals)
qqline(residuals)
```

It appears that the normality assumption is fine since the standardized residuals closely follow the theoretical quantiles for a normal distribution as observered in the quantile-quantile plot. 

### Part b.

```{r, out.width="75%"}
plot(fitted(model), residuals, main = "Residuals vs Fitted Value")
abline(h=0, col = 'red')
```

The model appears to be adequate since the residuals vs fitted values appear to be randomly distributed around 0, showing no patterns. 

### Part c.
```{r, out.width="75%"}
residuals <- residuals(model)
plot(tableb1$x2, residuals, xlab="Passing Yardage",
     ylab="Residuals", main="Residuals vs Team Passing Yardage")
abline(h=0, col="red")
```

The model appears to be improved when adding $x_2$ since the residuals appear to be more close to 0.  

## Lack of Fit Example

We are going to use this data to conduct a lack of fit test.

\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
      x & 1 & 1 & 2 & 3.3 & 3.3 & 4 & 4 & 4 & 4.7 & 5\\
      \hline
      y & 10.84 & 9.30 & 16.35 &22.88  &24.35 & 24.56& 25.86 &29.16 &24.59& 22.25\\
      \hline
      x & 5.6 & 5.6 & 5.6 & 6.0 & 6.0 & 6.5 & 6 & & & \\
      \hline
      y & 25.90 & 27.20 & 25.61 & 25.45 & 26.56 & 21.03 & 21.46 & & & \\
    \hline
  \end{tabular}
\end{center}

We can count that there are $m= 10$ levels, and $n=17$ observations, so 
\[\sum_{i=1}^n n_i = 2 + 1 + 3 + \cdots = 17\]
We can compute then fit a model with this data in R,
```{r}
x <- c(1,1,2,3.3,3.3,4,4,4,4.7,5,5.6,5.6,5.6,6,6,6.5,6)
y <- c(10.84 , 9.30 , 16.35 ,22.88  ,24.35 , 24.56, 25.86 ,29.16 
      ,24.59, 22.25,25.90 , 27.20 , 25.61 , 25.45 , 26.56 , 21.03 ,21.46)
model <- lm(formula = y ~ x)
print(model)
anova(model)
```

We get the linear model 
\[\hat{y}_i = 12.5323 + 2.316x_i\]

Then, we can compute lack of fit and pure error with 
\[SS_{PE} = \sum_{i=1}^m\sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2, SS_{LOF} = SSE - SS_{PE}\]
In R, we get 

```{r, message=FALSE}
library(EnvStats)
anovaPE(model)
```
Our $F$-statistic is 7.70, and the critical value is $F_{\alpha, m-2, n-m}$, which we can compute in R or look at a table.
```{r}
qf(0.05, 8, 7, lower.tail=FALSE)
```

Therefore, we reject the null hypothesis $H_0: E(y_i) = \beta_0 + \beta_1x_i$.

# Interprolating Values From Look-up Table

Say we want to find the $t$-value for $t_{\alpha_0}$ where $\alpha_0$ is not located on the look up table, we then choose the 2 closest $\alpha$ values, call them $\alpha_1, \alpha_2$, so that $\alpha_1 < \alpha_0 < \alpha_2$. Then, we use the following formula to interprolate the $t$-value as 
\[t_{\alpha_0} \approx t_{\alpha_1} + \frac{(\alpha_0 - \alpha_1)(t_{\alpha_2} - t_{\alpha_1})}{\alpha_2 - \alpha_1}\]

## Example. 

Suppose you have the critical value $\alpha_0 = 0.015$, the 2 closest values would be $\alpha_1 = 0.01$, and $\alpha_2 = 0.025$. So, we get 
\[t_{0.015} \approx \frac{(0.015 - 0.01)(t_{0.025} - t_{0.01})}{0.025 - 0.01}\]

```{r}
noint <- lm(formula = y~ 0)
print(noint)
anova(noint)
```

# Chapter 5 - Weighted Least Squared and Transformations 

## Example of checking for model adequecy with BP test

Using the delivery time data, we can examine the model adequecy of 

```{r}
delivery <- read.table("./Data/delivery.txt",header=TRUE, sep='\t')
X1 = delivery$Number.of.Cases
X2 = delivery$Distance
Y = delivery$Delivery.Time
fit = lm(Y~X1+X2, data=delivery)
par(mfrow=c(2,2))
plot(fit)
```

We can see in the qq-plot that the residuals do not follow the theoretical quantiles of a normal distribution, so there is likely an issue with the normality assumption of the model. The residuals vs fitted appear to have some pattern resembling a parabola. 

```{r, message=FALSE}
library(lmtest)
bptest(fit)
```

We can see that the p-value that it is low, so we reject the null hypothesis that the variance is constant. We can attempt to remede the issue with the residuals by applying a boxcox transformation. 

```{r, message=FALSE}
library(MASS)
boxcox(fit)
b <- boxcox(fit)
lambda <- b$x[which.max(boxcox(fit)$y)]
lambda
```

Now using our lambda value, we can transform $y$ and refit the model,

```{r}
n = length(Y)

y_dot <- exp(1/n * sum(log(Y)))

Y_transformed <- (Y^lambda - 1) / (lambda * y_dot^(lambda - 1))

new_model <- lm(Y_transformed ~ X1 + X2, data=delivery)

par(mfrow=c(2,2))

plot(new_model)

```

We can see that the qq-plot looks better now and the residuals appear fairly normally distributed, as well as the residuals vs fitted look more random, so we also can conclude that there is no longer an issue with the constant variance assumption. Another approach to this problem is to use another transformation on $y$. Since the reponse variable $y$ is the time, which is count, the simplest probabilistic model for count
data is the Poisson distribution, thus we transform $y' = \sqrt{y}$, and we plot this model. 

```{r}
y_prime <- sqrt(Y)

sqrt_y_model <- lm(y_prime ~ X1 + X2, data=delivery)

par(mfrow=c(2,2))

plot(sqrt_y_model)
```

We see that we get fairly similar results to the boxcox transformation, and can conlude the assumption of constant variance is no longer violated.


## Example of Weighted Least Squares

Using the weighted Turkey data, we can fit a model an examine the residuals. 

```{r}
turkey <- read.table("./Data/weighted.txt",header=TRUE, sep='\t')



fit <- lm(Y ~ X, data=turkey)

summary(fit)

plot(fit,1)
```

As we can see, there appears to be a telescoping effect. One way to proceed is to perform the usual regression. Then, group the data using the X variable. Estimate the variances $s^2_i$ of the $Y_i$ for each group. Then fit the variances against the averages of the $X_i$ of the groups. Next we computed averages and variances for subsets of the data and then fitted the variances against the averages.

```{r}
turkey
```

We can see that we have many data points at 3, 5.4, 7.8, 9.1, and 10.2. These aren't perfect groupings but there are many points at this $X$ value or very close to it, so we will use these as our groups. Now we can compute the variances at each of these points.


```{r}



s1 <- round(var(turkey[turkey$X == 3, ]$Y),4)
s2 <- round(var(subset(turkey, X >= 5.34 & X <= 5.45)$Y),4)
s3 <- round(var(subset(turkey, X >= 7.7 & X <= 7.94)$Y),4)
s4 <- round(var(subset(turkey, X >= 9.03 & X <= 9.37)$Y),4)
s5 <- round(var(subset(turkey, X >= 10.03 & X <= 10.5)$Y),4)

s <- c(s1, s2, s3, s4, s5)

df <- data.frame(X = c(3, 5.4, 7.8, 9.1, 10.2), s = s)
df
```

So now that we have our dataframe, we can fit a model with $s$ as the response and the averages that we picked, 

```{r}
fit2 <- lm(s ~ X + I(X^2), data=df)
summary(fit2)
```

Now, we have the regression model 

\[\hat{s}^2 = 1.5329 - 0.7334\bar{X} + 0.0883\bar{X}^2\]

The weights are then computes as inverses of the predicted variances,

```{r}
weights <- 1/predict(fit2, newdata = data.frame(X = turkey$X))

weighted_model <- lm(Y ~ X, data=turkey, weights=weights)
summary(weighted_model)
```

The original model was 


\[\hat{Y} = -0.579 + 1.14 X\]

The new weighted model becomes 

\[\hat{Y} = -0.89 + 1.16 X\]

# Chapter 6 - Regression Diagnostics and Measures of Influence

We will examine the bank dataset and fit a model to the data. Then examine the residuals and look for any outliers or influential points. 

```{r}
library(olsrr)
bank <- read.table("./Data/bank.txt",header=TRUE, sep='\t')

y <- bank$Number.New.accounts
x <- bank$Minimum.Deposit
fit <- lm(y~x)
# Cook's Distance vs. Observations
ols_plot_cooksd_bar(fit)
```

From the Cook's distant plot, we see that the 4th observation is influential on the fitted values at all $X$ values. We can look at the plot for DFFITS,

```{r}
ols_plot_dffits(fit)
```

We see again that the 4th observation is influential on the 4th fitted value of the model. We can also look at the DFBETAS plot, 

```{r}
ols_plot_dfbetas(fit)
```

From the DFBETAS vs Observation plots, we see that the 4th, 7th, and 10th observations are influential on the slope, and the 4th and 7th oberservations are influential on the intercept. To summarize, we can examine the measures of influence with 

```{r}
influence.measures(fit)
```

# Chapter 7 - Polynomial and Indicator Regression

## Example using Indicator Variables

We will use the turkey dataset 

```{r}
turkey <- read.table("./Data/turkey.txt",header=TRUE, sep='\t')
turkey
```

We can see that we have a catagorical variable "origin", which has 3 levels, G, V, and W. So, we create 2 dummy variables $Z_1$ and $Z_2$ to represent the levels. In this case, it was done for us with $(Z_1, Z_2) = (0,0) \implies W$, $(Z_1, Z_2) = (0,1) \implies V$, and $(Z_1, Z_2) = (1,0) \implies G$. 1 dummy variables allows for $2^1 = 2$ levels, 2 dummy variables gives us $2^{2} = 4$ levels, and so on. 

We can plot a model using just age and weight, 

```{r}
model1 <- lm(Weight ~ Age, data=turkey)
summary(model1) 
```

We see that there is a significant relationship between age and weight, the $R^2$ value is a bit low at 0.66, we can examine the model adequency. 

```{r, out.width="50%", echo=FALSE}
plot(model1,1)
plot(model1,2)
plot(model1,3)
plot(model1,4)
plot(model1,5)
plot(model1,6)
```

We can see from the qq-plot, that the normally assumption of the residuals may not be reasonable. We can also see from the residuals vs fitted plot that there is a curve in the data, which tells us that our linear model may not be a good fit. Using the criteria that an observation is influential when Cook's distance $D_i > 1$, we see there are no influential points.


Now we can try fitting the model with our 2 dummy variables to see if it improves,

```{r}
model2 <- lm(Weight ~ Age + Z1 + Z2, data=turkey)
summary(model2)
```

We see that all the predictors are significant, and our $R^2$ value has increased significantly. We can examine the model adequency again, 

```{r, out.width="50%", echo=FALSE}
plot(model2,1)
plot(model2,2)
plot(model2,3)
plot(model2,4)
plot(model2,5)
plot(model2,6)
```

The qq-plot shows the assumption of normally distributed residuals is more reasonable now, and the residuals vs fitted is looking more random, however there still is a slight curve which could require more adjustments. 

## Example with Polynomial Regression

For this example we will use the hardwood dataset, we'll start by fitting a simple model and examining its adequecy,

```{r}
hardwood <- read.table("./Data/hardwood.txt",header=TRUE, sep='\t')
x <- hardwood$Concentration
y <- hardwood$Tensile.Strength.Y
model1 <- lm(y ~ x)
par(mfrow=c(2,2))
plot(model1)
```

We can see from the residuals vs fitted, a very clear parabola shape is present, which indicates that there may be a non linear relationship our model is failing to account for. So, we will try to fit a quadratic model, 

```{r}
model2 <- lm(y ~ x + I(x^2))
par(mfrow=c(2,2))
plot(model2)
```

We can see now that the residuals vs fitted is slightly more random, however there is still a curve which indicates that there is evidence of a non linear relationship, and that the assumption of constant variance may not be reasonable. A higher order polynomial may be needed to fit the data better. We can also examine the differences in the $R^2$ values that we get from the 2 models, 

```{r}

summary(model1)$r.squared
summary(model2)$r.squared
```

As we can see, the $R^2$ value for the quadratic model is much higher, so we can conclude that the quadratic model is a better fit.

# Chapter 8 - Multicolinearity 

We will use the acetylene dataset for this example. 

```{r}
acetylene <- read.table("./Data/acetylene.txt",header=TRUE, sep='\t')
acetylene
```

We want to consider the full quadratic model which takes into account the interactions as well, and we want each of the regression to be scaled using the unit normal scaling (subtract mean and divide by standard deviation). 

```{r}
P <- acetylene$Acetylene
T <- scale(acetylene$Temperature)
H <- scale(acetylene$Ratio)
C <- scale(acetylene$Contact.Time)

model <- lm(P ~ T + H + C + I(T^2) + I(H^2) + I(C^2) + T*H + T*C + H*C)
summary(model)
```
This gives us our model 

\[\hat{P} = 35.8958 + 4.0038T + 2.7783 H -8.0423C -6.4568TH -26.9804TC\]
\[-3.7681HC -12.5236T^2 -0.9727H^2-11.5932C^2\]

We can examine the correlation between our predictors

```{r}
predictors <- c(T,H,C, T*H, T*C, H*C, T^2, H^2, C^2)

df <- data.frame(T, H, C, T*H, H*C, T*C, T^2, H^2, C^2)
round(cor(df), 3)
```
As we can see, there's high correlation between temperataure and contact time, and there are other large correlations between $x_1x_2$ and $x_2x_3$, $x_1x_3$ and $x_1^2$, and $x_1^2$ and $x_3^3$. This is not surprising since these variables are generated from the linear terms and involve highly correlated regressors $x_1$ and $x_3$.

We can compute the variance inflation factors for each of the predictors, 

```{r, message=FALSE}
library(car)
vif(model)
```

As we can see many of these have very high VIF, which certainly indicates multicollinearity. We can also compute the condition number $\kappa$ and condition indices $\kappa_j = \frac{\lambda_{\max}}{\lambda_j}$. Note that $\lambda_j$ are the eigenvalues for the matrix $X'X$.

```{r}
eigen_values <- eigen(cor(df))$values
eigen_values
kappa <- max(eigen_values) / min(eigen_values)
kappa
kappa_j <- max(eigen_values) / eigen_values
kappa_j
```

As we can see, the condition number $\kappa = 43381.31$ is very large which indicates severe multicollinearity. The condition indices greater than 1000 indicate severe multicollinearity, those between 100 and 1000 indicate moderate to strong multicollinearity, and those bellow 100 have no indication of multicollinearity. 

We can compute the tolerance values as well

```{r}
ols_coll_diag(model)$vif_t
```

Tolerance is the amount of variability in one independent variable that is not explained by the other independent variables. To compute it, regress the $k$th predictor on the other predictors in the model and compute the $R^2$ value, then Tolerance $= 1- R^2$. Tolerance values less than 0.10 indicate collinearity.

We can preform ridge regression now as well

```{r}
ridge_model <- lm.ridge(P ~ T + H + C + I(T^2) + I(H^2) + I(C^2)
                        + T*H + T*C + H*C, lambda=seq(0,0.1,0.01))
plot(ridge_model)

```

The y axis represents the coefficient $\hat{\beta}_R$ which is the solution to the equation $(X'X + cI)\hat{\beta}_R = X'Y$, the x-axis is those values of $c$ that we are looking for. We want to choose $c$ large enough to provide stable coefficients but not too large. Now we want to preform cross validation and obtain the best value for $c$.

```{r, message=FALSE, warning=FALSE}
library(glmnet)

x = data.matrix(df)
y = P
model <- glmnet(x,y, alpha=0)
cv_model <- cv.glmnet(x,y, alpha=0)
plot(cv_model)
```

Now we can extract the optimal value of lambda and create the best model, 

```{r}
lambda <- cv_model$lambda.min
ridge_model <- glmnet(x,y, alpha=0, lambda=lambda)
coef(ridge_model)
```