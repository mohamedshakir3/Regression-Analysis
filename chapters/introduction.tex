\chapter{Introduction} 

The primary goal in regression is to a devlop a model that relates a set of explanatory variables $X_1, \ldots, X_p$ to a response variable $Y$, then test the model and use it for inference and predicition. 

Given a set of $n$ pairs of data $Y_i$ and $X_i$, we attempt to fit a straight line to these points, using a simple regression model 
\[Y_i = \beta_0 + \beta_1X_i + \epsilon_i\]

Where $\epsilon_i$ represents an unobserved random error term, $\beta_0$ is the intercept and $\beta_1$ is the slope of the line. $\beta_0$ and $\beta_1$ are parameters that need to be estimated from observed data. The model can also be expressed in terms of $(X_i - \bar{X})$.

\[Y_i = (\beta_0 + \beta_1\bar{X}) + \beta_1(X_i - \bar{X}) + \epsilon_i\]

Where $\bar{X}$ is the sample mean 
\[\bar{X} = \frac{1}{n}\sum_{i=1}^n  X_i \]

This proposed model is linear in the parameters $\beta_0$, $\beta_1$, and would still be referred to as linear if we had $X_i^2$ instead of $X_i$. This model also makes the assumption that the random error terms $\epsilon_i$ are uncorrelated, have mean 0, and variance $\sigma^2$. Under these assumptions, we have 
\[E(Y_i) = \beta_0 + \beta_1 X_i\]

\[\Var(Y_i) = \sigma^2\]

